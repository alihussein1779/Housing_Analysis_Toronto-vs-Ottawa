{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "\n",
    "# For HTML parsing\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "\n",
    "# For website connections\n",
    "import requests \n",
    "\n",
    "# For data cleanup\n",
    "import re\n",
    "\n",
    "# For zipcode search\n",
    "#!pip install opencage\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "\n",
    "# To prevent overwhelming the server between connections\n",
    "import time\n",
    "from time import sleep \n",
    "\n",
    "# Display the progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# For creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_driver = webdriver.Chrome()\n",
    "\n",
    "# Function to collect raw data from url:\n",
    "\n",
    "def get_page(city, type, beds, page):\n",
    "  \n",
    "    url    = f'https://www.torontorentals.com/{city}/{type}?beds={beds}%20&p={page}'\n",
    "    result = requests.get(url)\n",
    "    # https://www.torontorentals.com/toronto/condos?beds=1%20&p=2\n",
    "    # check HTTP response status codes to find if HTTP request has been successfully completed\n",
    "    if result.status_code >= 100  and result.status_code <= 199:\n",
    "        print('Informational response')\n",
    "    if result.status_code >= 200  and result.status_code <= 299:\n",
    "        print('Successful response')\n",
    "        web_driver.get(url)\n",
    "        time.sleep(2)\n",
    "        web_driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(web_driver.page_source,'lxml')\n",
    "    if result.status_code >= 300  and result.status_code <= 399:\n",
    "        print('Redirect')\n",
    "    if result.status_code >= 400  and result.status_code <= 499:\n",
    "        print('Client error')\n",
    "    if result.status_code >= 500  and result.status_code <= 599:\n",
    "        print('Server error')\n",
    "        \n",
    "    return soup\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Data that will be used in the function\n",
    "house_type = [\"Apartment\",\"condo\",\"room\",\"house\",\"studio\",\"basement\"]\n",
    "bed_options = [\"0\",\"1\",\"2\",\"3\",\"4\",\"1-2\",\"1-3\"]\n",
    "\n",
    "# Lists that will contain the clean data\n",
    "listData = []\n",
    "listingStreet = []\n",
    "listingCity = []\n",
    "listingZip = []\n",
    "listingRent = []\n",
    "listingBed = []\n",
    "listingBath = []\n",
    "listingDim = []\n",
    "listingType = []\n",
    "ListingID = []\n",
    "\n",
    "# Code that implements the above function and the above lists to collect raw data          \n",
    "  \n",
    "for page_num in tqdm(range(1,100)):  # Range depends on how many pages you want to analyze\n",
    "    soup_page                = get_page('toronto', house_type, bed_options, page_num)\n",
    "            \n",
    "  \n",
    "  #Data Collection\n",
    "     \n",
    "    #This contains info on all datapoints needed, but will use other links instead to avoid mistakes during the clean up process\n",
    "    data = soup_page.find_all(\"div\",{\"class\":\"r-listing-card-v\"})\n",
    "    listData.append(data)  \n",
    "    \n",
    "    \n",
    "    # Street, Rent & House type had unique identifiers in the HTML \n",
    "    street                   = soup_page.find_all(\"div\",{\"class\":\"r-listing-address q-mb-md q-pl-md\"})\n",
    "    rent                     = soup_page.find_all(\"a\",{\"class\":\"r-listing-price q-my-md q-mr-md q-pl-md\"})\n",
    "    house_type               = soup_page.find_all(\"span\",{\"class\":\"r-listing-type\"})\n",
    "\n",
    "    # Bed, Bath and Dimensions had the same identifier from the HTML\n",
    "    data_bed_bath_dimensions = soup_page.find_all(\"span\",{\"class\":\"r-listing-infos__label\"})\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Data Cleanup # Appending to Lists\n",
    "    \n",
    "    # Street & House type had unique identifiers in the HTML \n",
    "\n",
    "    # Address\n",
    "    str_street = [str(item) for item in street]\n",
    "    cleaned_street = [sub.replace('<div class=\"r-listing-address q-mb-md q-pl-md\">',\"\")\n",
    "                        .replace('</div>',\"\") for sub in str_street]\n",
    "    for i in cleaned_street:\n",
    "        listingStreet.append(i)\n",
    "        \n",
    "    # House Type    \n",
    "    str_house_type = [str(item) for item in house_type]\n",
    "    cleaned_house_type = [sub.replace('<span class=\"r-listing-type\">',\"</span>,\")\n",
    "                            .replace('</span>',\"\").replace(\",\",\"\") for sub in str_house_type]\n",
    "    for i in cleaned_house_type:\n",
    "        listingType.append(i)    \n",
    "        \n",
    "        \n",
    "    # Price    \n",
    "    str_rent = [str(item) for item in rent]\n",
    "    rent_1 = [sub.replace('<a class=\"r-listing-price q-my-md q-mr-md q-pl-md\"','').replace('href=\"/toronto/','')\n",
    "                        .replace('</a>',\"\").replace(\",\",\"\") for sub in str_rent]\n",
    "    rent_2 = [item.split(\">\") for item in rent_1]\n",
    "    cleaned_rent = [' - '.join(item.split(' - ')[:2]) for _, item in rent_2]\n",
    "    for i in cleaned_rent:\n",
    "        listingRent.append(i)\n",
    "        \n",
    "    # Bed, Bath & Dimensions    \n",
    "    str_data_bed_bath_dimensions = [str(item) for item in data_bed_bath_dimensions]\n",
    "    cleaned_dimensions_bath_bed = [sub.replace('<span data-current-language=\"en-US\"',\"\")\n",
    "                            .replace('<span class=\"r-listing-infos__label\">',\"\")\n",
    "                            .replace('data-msgid=',\"\").replace('>bed</span></span>',\"\")\n",
    "                            .replace('>bath</span></span>',\"\").replace('>Ft</span></span>',\"\")\n",
    "                            for sub in str_data_bed_bath_dimensions]\n",
    "\n",
    "    combined_info = []\n",
    "    listingCombinedInfo = []\n",
    "    for i in range(0, len(cleaned_dimensions_bath_bed), 3):\n",
    "        bed = cleaned_dimensions_bath_bed[i].split()[0] if i < len(cleaned_dimensions_bath_bed) else \"N/A\"\n",
    "        bath = cleaned_dimensions_bath_bed[i + 1].split()[0] if i + 1 < len(cleaned_dimensions_bath_bed) else \"N/A\"\n",
    "        sqft = cleaned_dimensions_bath_bed[i + 2].split()[0] if i + 2 < len(cleaned_dimensions_bath_bed) else \"N/A\"\n",
    "\n",
    "        combined_info.append(f\"{bed} bed, {bath} bath, {sqft} ft\")\n",
    "        listingCombinedInfo.append(combined_info)\n",
    "\n",
    "    # Now combined_info contains the desired combined strings with the specified order and \"N/A\" for missing values.\n",
    "\n",
    "    for item in combined_info:\n",
    "        parts = item.split(', ')\n",
    "        \n",
    "        bed_part = parts[0].split()[0]\n",
    "        listingBed.append(bed_part)\n",
    "        \n",
    "        bath_part = parts[1].split()[0]\n",
    "        listingBath.append(bath_part)\n",
    "        \n",
    "        dim_part = parts[2].split()[0]\n",
    "        listingDim.append(dim_part)  \n",
    "        \n",
    "   \n",
    "    # For IDs:\n",
    "    \n",
    "    # Find all <a> elements with the class \"r-listing-price\"\n",
    "    anchor_elements = soup_page.find_all('a', class_='r-listing-price')\n",
    "\n",
    "    # Iterate through the <a> elements and extract the IDs\n",
    "    for anchor_element in anchor_elements:\n",
    "        href = anchor_element['href']\n",
    "\n",
    "        # Extract the ID from the href attribute using regular expressions\n",
    "        id_match = re.search(r'id(\\d+)', href)\n",
    "        if id_match:\n",
    "            listing_id = id_match.group(1)\n",
    "            ListingID.append(listing_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the zipcodes for addresses\n",
    "\n",
    "# Initialize the OpenCageGeocode API key\n",
    "api_key = \"4d9d18c5a56040578558ee2d57caaf6f\"  #personal API key\n",
    "\n",
    "# Initialize the geocoder\n",
    "geocoder = OpenCageGeocode(api_key)\n",
    "\n",
    "\n",
    "# Function to extract city and postal code from address\n",
    "def extract_city_and_zip(address):\n",
    "    parts = address.split('-')\n",
    "    if len(parts) > 1:\n",
    "        city_part = parts[1].strip()  # Get the part after the hyphen and remove leading/trailing spaces\n",
    "        city = city_part.split(',')[0].strip()  # Extract the city name before the comma and remove spaces\n",
    "        return city\n",
    "    return None\n",
    "\n",
    "# Search for the postal code for each address in the list and add to the list of dictionaries\n",
    "for address in listingStreet:\n",
    "    result = geocoder.geocode(address, countrycode=\"CA\")\n",
    "    if result and 'components' in result[0]:\n",
    "        components = result[0]['components']\n",
    "        postal_code = components.get('postcode', 'Postal code not found')\n",
    "        listingZip.append(postal_code)\n",
    "    else:\n",
    "        listingZip.append('Postal code not found')\n",
    "\n",
    "# Function to extract city and postal code from address\n",
    "def extract_city_and_zip(address):\n",
    "    parts = address.split('-')\n",
    "    if len(parts) > 1:\n",
    "        right_part = parts[1].strip()  # Get the part after the hyphen and remove leading/trailing spaces\n",
    "        city_and_province = right_part.split(',')  # Split by comma to separate city and province\n",
    "        if len(city_and_province) > 1:\n",
    "            city = city_and_province[0].strip()  # Extract the city name and remove spaces\n",
    "            return city\n",
    "    return None\n",
    "\n",
    "# Apply the function to each element in the ListingStreet list\n",
    "for address in listingStreet:\n",
    "    city = extract_city_and_zip(address)\n",
    "    if city:\n",
    "        listingCity.append(city)\n",
    "    else:\n",
    "        listingCity.append('City not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "920\n",
      "889\n",
      "889\n",
      "889\n",
      "920\n",
      "0\n",
      "920\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(listingCity))\n",
    "print(len(listingType))\n",
    "print(len(listingBed))\n",
    "print(len(listingBath))\n",
    "print(len(listingDim))\n",
    "print(len(listingStreet))\n",
    "print(len(listingZip))\n",
    "print(len(listingRent))\n",
    "print(len(listingCombinedInfo))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe and Consolidating Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Property Type Bedrooms Bathrooms Square Footage                                Address Zip code          Price\n",
      "City                                                                                                                  \n",
      "Toronto     apartment      0-3         2            886     200 Redpath Avenue  - Toronto , ON  M4P 1G4  $2570 - $4980\n",
      "Toronto     apartment      1-2         2           1050             57 Spadina  - Toronto , ON  M5R 2X3  $3377 - $4687\n",
      "Toronto     apartment      1-3       2.5           1332        131 Mill Street  - Toronto , ON  M5A 3C4  $2521 - $4892\n",
      "Toronto     apartment    0-2.5         2            847         18 Erskine Ave  - Toronto , ON  M4P 1Y5  $2615 - $4095\n",
      "Toronto     apartment    1-2.5         2            963     118 Balliol Street  - Toronto , ON  M4S 3C4  $2605 - $4005\n",
      "...               ...      ...       ...            ...                                    ...      ...            ...\n",
      "Toronto         condo     None      None           None         78 Carr Street  - Toronto , ON  M6J 2E8          $3200\n",
      "Toronto     apartment     None      None           None   2369 Danforth Avenue  - Toronto , ON  M4C 1L1          $2650\n",
      "Toronto         condo     None      None           None  33 Shore Breeze Drive  - Toronto , ON  M8V 1A2          $2650\n",
      "Toronto     apartment     None      None           None  131 Kenilworth Avenue  - Toronto , ON  M4L 3S6          $4200\n",
      "Toronto     apartment     None      None           None  129 Kenilworth Avenue  - Toronto , ON  M4L 3S6          $4200\n",
      "\n",
      "[920 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # Import numpy for NaN values\n",
    "\n",
    "# Find the maximum length among all lists\n",
    "max_length = max(len(listingCity), len(listingType), len(listingBed), len(listingBath),\n",
    "                 len(listingDim), len(listingStreet), len(listingZip), len(listingRent))\n",
    "\n",
    "# Pad the shorter lists with None or NaN to match the maximum length\n",
    "def pad_list(lst, length, pad_value=None):\n",
    "    if len(lst) < length:\n",
    "        return lst + [pad_value] * (length - len(lst))\n",
    "    else:\n",
    "        return lst\n",
    "\n",
    "listingCity = pad_list(listingCity, max_length)\n",
    "listingType = pad_list(listingType, max_length)\n",
    "listingBed = pad_list(listingBed, max_length)\n",
    "listingBath = pad_list(listingBath, max_length)\n",
    "listingDim = pad_list(listingDim, max_length)\n",
    "listingStreet = pad_list(listingStreet, max_length)\n",
    "listingZip = pad_list(listingZip, max_length)\n",
    "listingRent = pad_list(listingRent, max_length)\n",
    "\n",
    "# Create the DataFrame\n",
    "column_names = [\"City\", \"Property Type\", \"Bedrooms\", \"Bathrooms\", \"Square Footage\", \"Address\",\n",
    "                \"Zip code\", \"Price\"]\n",
    "data = {\n",
    "    \"City\": listingCity,\n",
    "    \"Property Type\": listingType,\n",
    "    \"Bedrooms\": listingBed,\n",
    "    \"Bathrooms\": listingBath,\n",
    "    \"Square Footage\": listingDim,\n",
    "    \"Address\": listingStreet,\n",
    "    \"Zip code\": listingZip,\n",
    "    \"Price\": listingRent\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the index (if needed)\n",
    "df.set_index('City', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\n1. Web Scraping Rentals Website Using Python Beautiful Soup: https://medium.com/swlh/web-scraping-rentals-website-using-beautiful-soup-and-pandas-99e255f27052\\n2. Chat GPT (For fine-tuning)\\n3. StackOverflow\\n4. Google\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "1. Web Scraping Rentals Website Using Python Beautiful Soup: https://medium.com/swlh/web-scraping-rentals-website-using-beautiful-soup-and-pandas-99e255f27052\n",
    "2. Chat GPT (For fine-tuning)\n",
    "3. StackOverflow\n",
    "4. Google\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a csv file to compare with Ottawa\n",
    "folder = r\"D:\\Projects\\Rentals/\"\n",
    "df.to_excel(\"rental_data_toronto_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
